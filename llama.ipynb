{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Using cached ollama-0.4.6-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from ollama) (2.9.2)\n",
      "Collecting httpx<0.28.0,>=0.27.0\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: certifi in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.12.14)\n",
      "Requirement already satisfied: sniffio in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
      "Requirement already satisfied: anyio in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.8.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/darth/.pyenv/versions/3.10.13/envs/LLLM/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.2)\n",
      "Installing collected packages: httpx, ollama\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "Successfully installed httpx-0.27.2 ollama-0.4.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': '',\n",
    "        'images': ['images/attention-images-0.jpg']\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '',\n",
    "        'images': ['images/attention-images-1.jpg']\n",
    "    },\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content':'list down all the sub topics in the files'\n",
    "    }\n",
    "    ],\n",
    "    stream = True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided image is a page from a research paper on deep learning, specifically focusing on recurrent neural networks (RNNs) and their application to natural language processing tasks.\n",
      "\n",
      "**Main Topics:**\n",
      "\n",
      "*   **Recurrent Neural Networks (RNNs):** The paper explores the use of RNNs for modeling sequential data such as text. It discusses various architectures, including Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), which are designed to handle long-term dependencies in data.\n",
      "*   **Attention Mechanism:** The authors introduce an attention mechanism that allows the model to focus on specific parts of the input sequence when generating outputs. This is particularly useful for tasks like machine translation, where understanding the context and relevance of different words or phrases can significantly improve performance.\n",
      "*   **Transformer Architecture:** A key contribution of the paper is the introduction of a new transformer architecture inspired by the self-attention mechanism used in RNNs. Unlike traditional RNN architectures that process data sequentially, this model processes all input sequences simultaneously using self-attention layers. This approach allows for parallelization and scalability improvements compared to sequential processing methods.\n",
      "*   **Training and Evaluation:** The authors discuss challenges associated with training deep models like these, including issues related to vanishing gradients during backpropagation. They propose techniques such as residual connections or layer normalization to mitigate these effects and improve model stability.\n",
      "*   **Applications and Future Directions:** Finally, the paper touches on potential applications of this architecture in areas like machine translation, question answering, and text summarization. It also suggests avenues for future research, including exploring different variants of the transformer structure and applying it to other NLP tasks.\n",
      "\n",
      "**Subtopics:**\n",
      "\n",
      "1.  **Recurrent Neural Networks (RNNs)**\n",
      "    *   Introduction to RNNs\n",
      "    *   Types of RNN architectures (LSTM, GRU)\n",
      "2.  **Attention Mechanism**\n",
      "    *   Overview of attention mechanism in RNNs\n",
      "    *   Applications and benefits of attention in NLP tasks\n",
      "3.  **Transformer Architecture**\n",
      "    *   Introduction to transformer architecture\n",
      "    *   Self-attention mechanism in transformers\n",
      "    *   Parallelization and scalability advantages\n",
      "4.  **Training and Evaluation Challenges**\n",
      "    *   Vanishing gradients during backpropagation\n",
      "    *   Techniques for mitigating these effects (residual connections, layer normalization)\n",
      "5.  **Applications and Future Directions**\n",
      "    *   Potential applications of transformer architecture in NLP\n",
      "    *   Avenues for future research\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
